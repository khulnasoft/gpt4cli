"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[20],{5690:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var i=t(4848),s=t(8453);const o={sidebar_position:3,sidebar_label:"Prompts"},l="Prompts",r={id:"core-concepts/prompts",title:"Prompts",description:"Sending Prompts",source:"@site/docs/core-concepts/prompts.md",sourceDirName:"core-concepts",slug:"/core-concepts/prompts",permalink:"/core-concepts/prompts",draft:!1,unlisted:!1,editUrl:"https://github.com/khulnasoft/gpt4cli/tree/main/docs/docs/core-concepts/prompts.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,sidebar_label:"Prompts"},sidebar:"tutorialSidebar",previous:{title:"Context",permalink:"/core-concepts/context-management"},next:{title:"Pending Changes",permalink:"/core-concepts/reviewing-changes"}},a={},c=[{value:"Sending Prompts",id:"sending-prompts",level:2},{value:"Plan Stream TUI",id:"plan-stream-tui",level:2},{value:"Task Prompts",id:"task-prompts",level:2},{value:"Conversational Prompts",id:"conversational-prompts",level:2},{value:"Stopping and Continuing",id:"stopping-and-continuing",level:2},{value:"Background Tasks",id:"background-tasks",level:2},{value:"Building Files",id:"building-files",level:2},{value:"Skipping builds / <code>gpt4cli build</code>",id:"skipping-builds--gpt4cli-build",level:3},{value:"Iterating on a Plan",id:"iterating-on-a-plan",level:2},{value:"Continue the convo",id:"continue-the-convo",level:3},{value:"Rewind and iterate",id:"rewind-and-iterate",level:3},{value:"Which is better?",id:"which-is-better",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"prompts",children:"Prompts"})}),"\n",(0,i.jsx)(n.h2,{id:"sending-prompts",children:"Sending Prompts"}),"\n",(0,i.jsxs)(n.p,{children:["To send a prompt to the model, use the ",(0,i.jsx)(n.code,{children:"gpt4cli tell"})," command."]}),"\n",(0,i.jsxs)(n.p,{children:["You can pass it in as a file with the ",(0,i.jsx)(n.code,{children:"--file/-f"})," flag:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli tell -f prompt.txt\n"})}),"\n",(0,i.jsx)(n.p,{children:"Write it in vim:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli tell # tell with no arguments opens vim so you can write your prompt there\n"})}),"\n",(0,i.jsx)(n.p,{children:"Or pass it inline (use enter for line breaks):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "add a new line chart showing the number of foobars over time to components/charts.tsx"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"plan-stream-tui",children:"Plan Stream TUI"}),"\n",(0,i.jsxs)(n.p,{children:["After you send a prompt with ",(0,i.jsx)(n.code,{children:"gpt4cli tell"}),", you'll see the plan stream TUI. The model's responses are streamed here. You'll see several hotkeys listed along the bottom row that allow you to stop the plan (s), send the plan to the background (b), scroll/page the streamed text, or jump to the beginning or end of the stream. If you're a vim user, you'll notice Gpt4cli's scrolling hotkeys are the same as vim's."]}),"\n",(0,i.jsx)(n.p,{children:"Note that scrolling the terminal window itself won't work while you're in the stream TUI. Use the scroll hotkeys instead."}),"\n",(0,i.jsx)(n.h2,{id:"task-prompts",children:"Task Prompts"}),"\n",(0,i.jsxs)(n.p,{children:["The most common prompt is a ",(0,i.jsx)(n.strong,{children:"task"}),". When you give Gpt4cli a task, it will first break down the task into steps, then it will proceed to implement each step in code. Gpt4cli will automatically continue sending model requests until the task is determined to be complete."]}),"\n",(0,i.jsx)(n.h2,{id:"conversational-prompts",children:"Conversational Prompts"}),"\n",(0,i.jsx)(n.p,{children:"You can also ask Gpt4cli questions or chat with it about whatever you want:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "explain every function in lib/math.ts"\n'})}),"\n",(0,i.jsx)(n.p,{children:"For conversational prompts, Gpt4cli will reply with just a single response and won't automatically continue."}),"\n",(0,i.jsx)(n.p,{children:"Note that Gpt4cli can sometimes be a bit over-eager to interpret conversational prompts as tasks. Its built-in prompts generally give it a bias toward action. For example, responding to a prompt like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "did you forget to add the submit button?"\n'})}),"\n",(0,i.jsx)(n.p,{children:"Gpt4cli is likely to respond by implementing the missing button rather than simply answering the question. Giving an additional nudge to clarify that you're only chatting and aren't implying that you want a task to be done can help."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "did you forget to add the submit button? don\'t add it--just answer yes or no."\n'})}),"\n",(0,i.jsx)(n.h2,{id:"stopping-and-continuing",children:"Stopping and Continuing"}),"\n",(0,i.jsxs)(n.p,{children:["You can prevent Gpt4cli from automatically continuing for multiple responses by passing the ",(0,i.jsx)(n.code,{children:"--stop/-s"})," flag:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell -s "write tests for the charting helpers in lib/chart-helpers.ts"\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Gpt4cli will then reply with just a single response. From there, you can continue if desired with the ",(0,i.jsx)(n.code,{children:"continue"})," command. Like ",(0,i.jsx)(n.code,{children:"tell"}),", ",(0,i.jsx)(n.code,{children:"continue"})," can also accept a ",(0,i.jsx)(n.code,{children:"--stop/-s"})," flag. Without the ",(0,i.jsx)(n.code,{children:"--stop/-s"})," flag, ",(0,i.jsx)(n.code,{children:"continue"})," will also cause Gpt4cli to continue automatically until the task is done. If you pass the ",(0,i.jsx)(n.code,{children:"--stop/-s"})," flag, it will continue for just one more response."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli continue -s\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Apart from ",(0,i.jsx)(n.code,{children:"--stop/-s"})," Gpt4cli's plan stream TUI also has an ",(0,i.jsx)(n.code,{children:"s"})," hotkey that allows you to immediately stop a plan."]}),"\n",(0,i.jsx)(n.h2,{id:"background-tasks",children:"Background Tasks"}),"\n",(0,i.jsxs)(n.p,{children:["By default, ",(0,i.jsx)(n.code,{children:"gpt4cli tell"})," opens the plan stream TUI and streams Gpt4cli's response(s) there, but you can also pass the ",(0,i.jsx)(n.code,{children:"--bg"})," flag to run a task in the background instead."]}),"\n",(0,i.jsxs)(n.p,{children:["You can learn more about using and interacting with background tasks ",(0,i.jsx)(n.a,{href:"/core-concepts/background-tasks",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"building-files",children:"Building Files"}),"\n",(0,i.jsxs)(n.p,{children:["As Gpt4cli implements your task, files it creates or updates will appear in the ",(0,i.jsx)(n.code,{children:"Building Plan"})," section of the plan stream TUI. Gpt4cli will ",(0,i.jsx)(n.strong,{children:"build"})," all changes proposed by the plan into a set of pending changesets for each affected file."]}),"\n",(0,i.jsxs)(n.p,{children:["Keep in mind that initially, these changes ",(0,i.jsx)(n.strong,{children:"will not"})," be directly applied to your project files. Instead, they will be ",(0,i.jsx)(n.strong,{children:"pending"})," in Gpt4cli's version-controlled sandbox. This allows you to review the proposed changes or continue iterating and accumulating more changes. You can view the pending changes with ",(0,i.jsx)(n.code,{children:"gpt4cli diff"})," (for git diff format) or ",(0,i.jsx)(n.code,{children:"gpt4cli changes"})," (to view them in a TUI). Once you're happy with the changes, you can apply them to your project files with ",(0,i.jsx)(n.code,{children:"gpt4cli apply"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/core-concepts/reviewing-changes",children:"Learn more about reviewing changes."})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/core-concepts/version-control",children:"Learn more about version control."})}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"skipping-builds--gpt4cli-build",children:["Skipping builds / ",(0,i.jsx)(n.code,{children:"gpt4cli build"})]}),"\n",(0,i.jsxs)(n.p,{children:["You can skip building files when you send a prompt by passing the ",(0,i.jsx)(n.code,{children:"--no-build"})," flag to ",(0,i.jsx)(n.code,{children:"gpt4cli tell"})," or ",(0,i.jsx)(n.code,{children:"gpt4cli continue"}),". This can be useful if you want to ensure that a plan is on the right track before comitting the time and tokens to build files."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "implement sign up and sign in forms in src/components" --no-build\n'})}),"\n",(0,i.jsxs)(n.p,{children:["You can later build any changes that were implemented in the plan with the ",(0,i.jsx)(n.code,{children:"gpt4cli build"})," command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli build\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This will show a smaller version of the plan stream TUI that only includes the ",(0,i.jsx)(n.code,{children:"Building Plan"})," section."]}),"\n",(0,i.jsxs)(n.p,{children:["Like full plan streams, build streams can stopped with the ",(0,i.jsx)(n.code,{children:"s"})," hotkey or sent to the background with the ",(0,i.jsx)(n.code,{children:"b"})," hotkey. They can also be run fully in the background with the ",(0,i.jsx)(n.code,{children:"--bg"})," flag:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli build --bg\n"})}),"\n",(0,i.jsxs)(n.p,{children:["There's one more thing to keep in mind about builds. If you send a prompt with the ",(0,i.jsx)(n.code,{children:"--no-build"})," flag:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "implement a forgot password email in src/emails" --no-build\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Then you later send ",(0,i.jsx)(n.em,{children:"another"})," prompt with ",(0,i.jsx)(n.code,{children:"gpt4cli tell"})," or continue the plan with ",(0,i.jsx)(n.code,{children:"gpt4cli continue"})," and you ",(0,i.jsx)(n.em,{children:"don't"})," include the ",(0,i.jsx)(n.code,{children:"--no-build"})," flag, any changes that were implemented previously but weren't built will immediately start building when the plan stream begins."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "now implement the UI portion of the forgot password flow" \n# the above will start building the changes proposed in the earlier prompt that was passed --no-build\n'})}),"\n",(0,i.jsx)(n.h2,{id:"iterating-on-a-plan",children:"Iterating on a Plan"}),"\n",(0,i.jsx)(n.p,{children:"If you send a prompt:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "implement a fully working and production-ready tic tac toe game, including a computer-controlled AI, in html, css, and javascript"\n'})}),"\n",(0,i.jsx)(n.p,{children:"And then you want to iterate on it, whether that's to add more functionality or correct something that went off track, you have a couple options."}),"\n",(0,i.jsx)(n.h3,{id:"continue-the-convo",children:"Continue the convo"}),"\n",(0,i.jsxs)(n.p,{children:["The most straightforward way to continue iterating is to simply send another ",(0,i.jsx)(n.code,{children:"gpt4cli tell"})," command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'gpt4cli tell "I plan to seek VC funding for this game, so please implement a dark mode toggle and give all buttons subtle gradient fills"\n'})}),"\n",(0,i.jsx)(n.p,{children:"This is generally a good approach when you're happy with the current plan and want to extend it to add more functionality."}),"\n",(0,i.jsxs)(n.p,{children:["Note, you can view the full conversation history with the ",(0,i.jsx)(n.code,{children:"gpt4cli convo"})," command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli convo\n"})}),"\n",(0,i.jsx)(n.h3,{id:"rewind-and-iterate",children:"Rewind and iterate"}),"\n",(0,i.jsxs)(n.p,{children:["Another option is to use Gpt4cli's ",(0,i.jsx)(n.a,{href:"/core-concepts/version-control",children:"version control"})," features to rewind to the point just before your prompt was sent and then update it before sending the prompt again."]}),"\n",(0,i.jsxs)(n.p,{children:["You can use ",(0,i.jsx)(n.code,{children:"gpt4cli log"})," to see the plan's history and determine which step to rewind to, then ",(0,i.jsx)(n.code,{children:"gpt4cli rewind"})," with the appropriate hash to rewind to that step:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli log # see the history\ngpt4cli rewind accfe9 # rewind to right before your prompt\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This approach works well in conjunction with ",(0,i.jsx)(n.strong,{children:"prompt files"}),". You write your prompts in files somewhere in your codebase, then pass those to ",(0,i.jsx)(n.code,{children:"gpt4cli tell"})," using the ",(0,i.jsx)(n.code,{children:"--file/-f"})," flag:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"gpt4cli tell -f prompts/tic-tac-toe.txt\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This makes it easy to continuously iterate on your prompt using ",(0,i.jsx)(n.code,{children:"gpt4cli rewind"})," and ",(0,i.jsx)(n.code,{children:"gpt4cli tell"})," until you get a result that you're happy with."]}),"\n",(0,i.jsx)(n.h3,{id:"which-is-better",children:"Which is better?"}),"\n",(0,i.jsxs)(n.p,{children:["There's not necessarily one right answer on whether to use an ongoing conversation or the ",(0,i.jsx)(n.code,{children:"rewind"})," approach with prompt files for iteration. Here are a few things to consider when making the choice:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Bad results tend to beget more bad results. Rewinding and iterating on the prompt is often more effective for correcting a wayward task than continuing to send more ",(0,i.jsx)(n.code,{children:"tell"})," commands. Even if you are specifically prompting the model to ",(0,i.jsx)(n.em,{children:"correct"})," a problem, having the wrong approach in its context will tend to bias it toward additional errors. Using ",(0,i.jsx)(n.code,{children:"rewind"})," to the give the model a clean slate can work better in these scenarios."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Iterating on a prompt file with the ",(0,i.jsx)(n.code,{children:"rewind"})," approach until you find your way to an effective prompt has another benefit: you can keep the final version of the prompt that produced a given set of changes right alongside the changes themselves in your codebase. This can be helpful for other developers (or your future self) if you want to revisit a task later."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["A downside of the ",(0,i.jsx)(n.code,{children:"rewind"})," approach is that it can involve re-running early steps of a plan over and over, which can be ",(0,i.jsx)(n.strong,{children:"a lot"})," more expensive than iterating with additional ",(0,i.jsx)(n.code,{children:"tell"})," commands."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function l(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);